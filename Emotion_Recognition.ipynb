{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AujYX0ldxjoU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import utils\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout,Flatten, Conv2D\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import SVG, Image\n",
        "from livelossplot.inputs.tf_keras import PlotLossesCallback\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for expression in os.listdir(\"PATH OF TRAIN FOLDER\"):\n",
        "    print(str(len(os.listdir(\"PATH OF TRAIN FOLDER\" + expression))) + \" \" + expression + \" images\")"
      ],
      "metadata": {
        "id": "ZsNmmzWWxm0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = 48\n",
        "batch_size = 64\n",
        "datagen_train = ImageDataGenerator(horizontal_flip=True)\n",
        "train_generator = datagen_train.flow_from_directory(\"PATH OF TRAIN FOLDER\",\n",
        "                                                    target_size=(img_size,img_size),\n",
        "                                                    color_mode=\"grayscale\",\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    shuffle=True)\n",
        "datagen_validation = ImageDataGenerator(horizontal_flip=True)\n",
        "validation_generator = datagen_validation.flow_from_directory(\"PATH OF TEST FOLDER\",\n",
        "                                                    target_size=(img_size,img_size),\n",
        "                                                    color_mode=\"grayscale\",\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    shuffle=False)"
      ],
      "metadata": {
        "id": "H-QVqw7Pxpuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialising the CNN\n",
        "model = Sequential()\n",
        "# 1 - Convolution\n",
        "model.add(Conv2D(64,(3,3), padding='same', input_shape=(48, 48,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "# 2nd Convolution layer\n",
        "model.add(Conv2D(128,(5,5), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "# 3rd Convolution layer\n",
        "model.add(Conv2D(512,(3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "# 4th Convolution layer\n",
        "model.add(Conv2D(512,(3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "# Flattening\n",
        "model.add(Flatten())\n",
        "# Fully connected layer 1st layer\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "# Fully connected layer 2nd layer\n",
        "model.add(Dense(512))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "opt = Adam(lr=0.0005)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "6ZacmYlbxr-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
        "Image('model.png',width=400, height=200)"
      ],
      "metadata": {
        "id": "a5AAA9FFxugd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "epochs = 15\n",
        "steps_per_epoch = train_generator.n//train_generator.batch_size\n",
        "validation_steps = validation_generator.n//validation_generator.batch_size\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
        "                              patience=2, min_lr=0.00001, mode='auto')\n",
        "checkpoint = ModelCheckpoint(\"model_weights.h5\", monitor='val_accuracy',\n",
        "                             save_weights_only=True, mode='max', verbose=1)\n",
        "callbacks = [PlotLossesCallback(), checkpoint, reduce_lr]\n",
        "history = model.fit(\n",
        "    x=train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=epochs,\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps = validation_steps,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "fq-0DF4-xxbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_json = model.to_json()\n",
        "model.save_weights('model_weights.h5')\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)"
      ],
      "metadata": {
        "id": "t5yc1Ax4xziP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import model_from_json\n",
        "class FacialExpressionModel(object):\n",
        "    EMOTIONS_LIST = [\"Angry\", \"Disgust\",\n",
        "                    \"Fear\", \"Happy\",\n",
        "                    \"Neutral\", \"Sad\",\n",
        "                    \"Surprise\"]\n",
        "    def __init__(self, model_json_file, model_weights_file):\n",
        "        # load model from JSON file\n",
        "        with open(model_json_file, \"r\") as json_file:\n",
        "            loaded_model_json = json_file.read()\n",
        "            self.loaded_model = model_from_json(loaded_model_json)\n",
        "        # load weights into the new model\n",
        "        self.loaded_model.load_weights(model_weights_file)\n",
        "        self.loaded_model.make_predict_function()\n",
        "    def predict_emotion(self, img):\n",
        "        self.preds = self.loaded_model.predict(img)\n",
        "        return FacialExpressionModel.EMOTIONS_LIST[np.argmax(self.preds)]"
      ],
      "metadata": {
        "id": "UUSXqSiNx6cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "facec = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "model = FacialExpressionModel(\"model.json\", \"model_weights.h5\")\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "class VideoCamera(object):\n",
        "    def __init__(self):\n",
        "        self.video = cv2.VideoCapture(0)\n",
        "    def __del__(self):\n",
        "        self.video.release()\n",
        "    # returns camera frames along with bounding boxes and predictions\n",
        "    def get_frame(self):\n",
        "        _, fr = self.video.read()\n",
        "        gray_fr = cv2.cvtColor(fr, cv2.COLOR_BGR2GRAY)\n",
        "        faces = facec.detectMultiScale(gray_fr, 1.3, 5)\n",
        "        for (x, y, w, h) in faces:\n",
        "            fc = gray_fr[y:y+h, x:x+w]\n",
        "            roi = cv2.resize(fc, (48, 48))\n",
        "            pred = model.predict_emotion(roi[np.newaxis, :, :, np.newaxis])\n",
        "            cv2.putText(fr, pred, (x, y), font, 1, (255, 255, 0), 2)\n",
        "            cv2.rectangle(fr,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "        return fr"
      ],
      "metadata": {
        "id": "DBLOMfbix9zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen(camera):\n",
        "    while True:\n",
        "        frame = camera.get_frame()\n",
        "        cv2.imshow('Facial Expression Recognization',frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "ldtg6VZ2x-kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen(VideoCamera())"
      ],
      "metadata": {
        "id": "optad5uOyCf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jmdxY_m7yD8P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}